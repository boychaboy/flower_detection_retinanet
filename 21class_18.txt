Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to ./resnet18-5c106cde.pth
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%CUDA available: True
Creating a model named : 21class_18...
Num training images: 1627

2020-06-26 12:58:20,605 - retinanet_logs - INFO - Epoch: 0 | Iteration: 0 | Classification loss: 1.15032 | Regression loss: 1.05343 | Running loss: 2.20374
2020-06-26 12:58:31,611 - retinanet_logs - INFO - Epoch: 0 | Iteration: 20 | Classification loss: 1.20874 | Regression loss: 0.90120 | Running loss: 2.14572
2020-06-26 12:58:42,565 - retinanet_logs - INFO - Epoch: 0 | Iteration: 40 | Classification loss: 1.16710 | Regression loss: 0.83624 | Running loss: 2.08718
2020-06-26 12:58:53,647 - retinanet_logs - INFO - Epoch: 0 | Iteration: 60 | Classification loss: 1.08905 | Regression loss: 0.52458 | Running loss: 2.01191
2020-06-26 12:59:04,996 - retinanet_logs - INFO - Epoch: 0 | Iteration: 80 | Classification loss: 0.76353 | Regression loss: 0.60608 | Running loss: 1.90309
2020-06-26 12:59:16,249 - retinanet_logs - INFO - Epoch: 0 | Iteration: 100 | Classification loss: 0.65987 | Regression loss: 0.57448 | Running loss: 1.79326
2020-06-26 12:59:28,160 - retinanet_logs - INFO - Epoch: 0 | Iteration: 120 | Classification loss: 0.52372 | Regression loss: 0.69114 | Running loss: 1.70212
2020-06-26 12:59:39,842 - retinanet_logs - INFO - Epoch: 0 | Iteration: 140 | Classification loss: 0.58577 | Regression loss: 0.31472 | Running loss: 1.61956
2020-06-26 12:59:52,166 - retinanet_logs - INFO - Epoch: 0 | Iteration: 160 | Classification loss: 0.55307 | Regression loss: 0.57311 | Running loss: 1.56189
2020-06-26 13:00:04,389 - retinanet_logs - INFO - Epoch: 0 | Iteration: 180 | Classification loss: 0.57544 | Regression loss: 0.58528 | Running loss: 1.51539
2020-06-26 13:00:16,666 - retinanet_logs - INFO - Epoch: 0 | Iteration: 200 | Classification loss: 0.78531 | Regression loss: 0.74889 | Running loss: 1.47598
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.1547312271062271
rose: 0.0
sunflower: 0.24621212121212122
lotus: 0.0
carnation: 0.0
daffodill: 0.0
magnolia: 0.0
camellia: 0.0
dandelion: 0.0
daisy: 0.0
mugungwha: 0.0
amalea: 0.0030303030303030307
moran: 0.002702702702702703
cosmos: 0.0
lily: 0.0
pasqueflower: 0.129743075214104
morning glory: 0.16249999999999998
wild rose: 0.0
hydrangea: 0.0
droplet: 0.1807124428718856
freesia: 0.0
2020-06-26 13:00:28,368 - retinanet_logs - INFO - Average accuracy : 0.04
2020-06-26 13:00:28,368 - retinanet_logs - INFO - Average loss 1.47:
2020-06-26 13:00:29,652 - retinanet_logs - INFO - Epoch: 1 | Iteration: 0 | Classification loss: 0.60444 | Regression loss: 0.60226 | Running loss: 1.46967
2020-06-26 13:00:42,037 - retinanet_logs - INFO - Epoch: 1 | Iteration: 20 | Classification loss: 0.64921 | Regression loss: 0.67300 | Running loss: 1.42710
2020-06-26 13:00:53,848 - retinanet_logs - INFO - Epoch: 1 | Iteration: 40 | Classification loss: 0.55035 | Regression loss: 0.56451 | Running loss: 1.39045
2020-06-26 13:01:05,971 - retinanet_logs - INFO - Epoch: 1 | Iteration: 60 | Classification loss: 0.42761 | Regression loss: 0.42839 | Running loss: 1.35797
2020-06-26 13:01:18,158 - retinanet_logs - INFO - Epoch: 1 | Iteration: 80 | Classification loss: 0.42110 | Regression loss: 0.28906 | Running loss: 1.33047
2020-06-26 13:01:30,431 - retinanet_logs - INFO - Epoch: 1 | Iteration: 100 | Classification loss: 0.59092 | Regression loss: 0.64406 | Running loss: 1.31494
2020-06-26 13:01:42,933 - retinanet_logs - INFO - Epoch: 1 | Iteration: 120 | Classification loss: 0.49424 | Regression loss: 0.55485 | Running loss: 1.29570
2020-06-26 13:01:54,618 - retinanet_logs - INFO - Epoch: 1 | Iteration: 140 | Classification loss: 0.40028 | Regression loss: 0.28495 | Running loss: 1.27316
2020-06-26 13:02:06,951 - retinanet_logs - INFO - Epoch: 1 | Iteration: 160 | Classification loss: 0.45297 | Regression loss: 0.47827 | Running loss: 1.25693
2020-06-26 13:02:18,479 - retinanet_logs - INFO - Epoch: 1 | Iteration: 180 | Classification loss: 0.63607 | Regression loss: 0.45244 | Running loss: 1.24221
2020-06-26 13:02:30,784 - retinanet_logs - INFO - Epoch: 1 | Iteration: 200 | Classification loss: 0.60270 | Regression loss: 0.58155 | Running loss: 1.22710
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.4
rose: 0.0
sunflower: 0.3
lotus: 0.30761904761904757
carnation: 0.0
daffodill: 0.7602272727272728
magnolia: 0.0
camellia: 0.39653700466200464
dandelion: 0.40522388059701486
daisy: 0.0
mugungwha: 0.0
amalea: 0.0
moran: 0.0
cosmos: 0.0
lily: 0.21449635932394556
pasqueflower: 0.4081326781326781
morning glory: 0.5138047138047138
wild rose: 0.3876984126984127
hydrangea: 0.37534517931619915
droplet: 0.5350350140056024
freesia: 0.38074534161490686
2020-06-26 13:02:41,715 - retinanet_logs - INFO - Average accuracy : 0.26
2020-06-26 13:02:41,715 - retinanet_logs - INFO - Average loss 0.98:
2020-06-26 13:02:43,027 - retinanet_logs - INFO - Epoch: 2 | Iteration: 0 | Classification loss: 0.47859 | Regression loss: 0.45816 | Running loss: 1.22401
2020-06-26 13:02:54,914 - retinanet_logs - INFO - Epoch: 2 | Iteration: 20 | Classification loss: 0.53653 | Regression loss: 0.42045 | Running loss: 1.20695
2020-06-26 13:03:06,828 - retinanet_logs - INFO - Epoch: 2 | Iteration: 40 | Classification loss: 0.38747 | Regression loss: 0.38586 | Running loss: 1.19132
2020-06-26 13:03:19,012 - retinanet_logs - INFO - Epoch: 2 | Iteration: 60 | Classification loss: 0.34363 | Regression loss: 0.31548 | Running loss: 1.17671
2020-06-26 13:03:31,256 - retinanet_logs - INFO - Epoch: 2 | Iteration: 80 | Classification loss: 0.42226 | Regression loss: 0.36437 | Running loss: 1.16268
2020-06-26 13:03:43,552 - retinanet_logs - INFO - Epoch: 2 | Iteration: 100 | Classification loss: 0.36226 | Regression loss: 0.50164 | Running loss: 1.13039
2020-06-26 13:03:55,586 - retinanet_logs - INFO - Epoch: 2 | Iteration: 120 | Classification loss: 0.46366 | Regression loss: 0.45621 | Running loss: 1.08083
2020-06-26 13:04:07,630 - retinanet_logs - INFO - Epoch: 2 | Iteration: 140 | Classification loss: 0.31688 | Regression loss: 0.32996 | Running loss: 1.03271
2020-06-26 13:04:19,203 - retinanet_logs - INFO - Epoch: 2 | Iteration: 160 | Classification loss: 0.43862 | Regression loss: 0.37305 | Running loss: 0.99526
2020-06-26 13:04:30,821 - retinanet_logs - INFO - Epoch: 2 | Iteration: 180 | Classification loss: 0.40475 | Regression loss: 0.35892 | Running loss: 0.96843
2020-06-26 13:04:42,442 - retinanet_logs - INFO - Epoch: 2 | Iteration: 200 | Classification loss: 0.37207 | Regression loss: 0.45917 | Running loss: 0.94680
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.9769230769230769
rose: 0.4291666666666667
sunflower: 0.6
lotus: 0.42222222222222217
carnation: 0.1
daffodill: 0.8531746031746033
magnolia: 0.0
camellia: 0.47619047619047616
dandelion: 0.42083333333333334
daisy: 0.9533333333333334
mugungwha: 0.4691666666666667
amalea: 0.0
moran: 0.0
cosmos: 0.1
lily: 0.21666666666666667
pasqueflower: 0.20606060606060606
morning glory: 0.7175
wild rose: 0.6476190476190476
hydrangea: 0.35787468550298207
droplet: 0.5709380809380808
freesia: 0.6148809523809524
2020-06-26 13:04:53,428 - retinanet_logs - INFO - Average accuracy : 0.43
2020-06-26 13:04:53,428 - retinanet_logs - INFO - Average loss 0.82:
2020-06-26 13:04:54,724 - retinanet_logs - INFO - Epoch: 3 | Iteration: 0 | Classification loss: 0.27257 | Regression loss: 0.42164 | Running loss: 0.94404
2020-06-26 13:05:06,817 - retinanet_logs - INFO - Epoch: 3 | Iteration: 20 | Classification loss: 0.39028 | Regression loss: 0.43583 | Running loss: 0.92931
2020-06-26 13:05:18,991 - retinanet_logs - INFO - Epoch: 3 | Iteration: 40 | Classification loss: 0.34394 | Regression loss: 0.26716 | Running loss: 0.91051
2020-06-26 13:05:31,172 - retinanet_logs - INFO - Epoch: 3 | Iteration: 60 | Classification loss: 0.29863 | Regression loss: 0.43378 | Running loss: 0.89141
2020-06-26 13:05:43,110 - retinanet_logs - INFO - Epoch: 3 | Iteration: 80 | Classification loss: 0.28204 | Regression loss: 0.52709 | Running loss: 0.87714
2020-06-26 13:05:54,862 - retinanet_logs - INFO - Epoch: 3 | Iteration: 100 | Classification loss: 0.37822 | Regression loss: 0.42697 | Running loss: 0.86177
2020-06-26 13:06:06,085 - retinanet_logs - INFO - Epoch: 3 | Iteration: 120 | Classification loss: 0.32825 | Regression loss: 0.33403 | Running loss: 0.85060
2020-06-26 13:06:18,128 - retinanet_logs - INFO - Epoch: 3 | Iteration: 140 | Classification loss: 0.25785 | Regression loss: 0.42380 | Running loss: 0.84126
2020-06-26 13:06:30,403 - retinanet_logs - INFO - Epoch: 3 | Iteration: 160 | Classification loss: 0.33209 | Regression loss: 0.60322 | Running loss: 0.83251
2020-06-26 13:06:42,222 - retinanet_logs - INFO - Epoch: 3 | Iteration: 180 | Classification loss: 0.24284 | Regression loss: 0.46893 | Running loss: 0.82119
2020-06-26 13:06:53,446 - retinanet_logs - INFO - Epoch: 3 | Iteration: 200 | Classification loss: 0.26323 | Regression loss: 0.39562 | Running loss: 0.80640
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.89
rose: 0.815919701213819
sunflower: 0.9
lotus: 0.4
carnation: 0.8
daffodill: 0.9007815713698066
magnolia: 0.5443640965380097
camellia: 0.788888888888889
dandelion: 0.6210719105455947
daisy: 1.0
mugungwha: 0.5352136752136752
amalea: 0.20404040404040405
moran: 0.1
cosmos: 0.16666666666666669
lily: 0.3524469950581859
pasqueflower: 0.5
morning glory: 0.5085106382978724
wild rose: 0.7311111111111112
hydrangea: 0.537506967670011
droplet: 0.7205128205128205
freesia: 0.1
2020-06-26 13:07:04,249 - retinanet_logs - INFO - Average accuracy : 0.58
2020-06-26 13:07:04,249 - retinanet_logs - INFO - Average loss 0.72:
2020-06-26 13:07:05,224 - retinanet_logs - INFO - Epoch: 4 | Iteration: 0 | Classification loss: 0.20197 | Regression loss: 0.27111 | Running loss: 0.80258
2020-06-26 13:07:16,925 - retinanet_logs - INFO - Epoch: 4 | Iteration: 20 | Classification loss: 0.34122 | Regression loss: 0.27115 | Running loss: 0.79076
2020-06-26 13:07:28,817 - retinanet_logs - INFO - Epoch: 4 | Iteration: 40 | Classification loss: 0.31272 | Regression loss: 0.45022 | Running loss: 0.77914
2020-06-26 13:07:40,854 - retinanet_logs - INFO - Epoch: 4 | Iteration: 60 | Classification loss: 0.29236 | Regression loss: 0.33966 | Running loss: 0.76700
2020-06-26 13:07:52,904 - retinanet_logs - INFO - Epoch: 4 | Iteration: 80 | Classification loss: 0.34045 | Regression loss: 0.44963 | Running loss: 0.75631
2020-06-26 13:08:04,792 - retinanet_logs - INFO - Epoch: 4 | Iteration: 100 | Classification loss: 0.15395 | Regression loss: 0.23726 | Running loss: 0.74725
2020-06-26 13:08:16,343 - retinanet_logs - INFO - Epoch: 4 | Iteration: 120 | Classification loss: 0.35078 | Regression loss: 0.36894 | Running loss: 0.73944
2020-06-26 13:08:28,219 - retinanet_logs - INFO - Epoch: 4 | Iteration: 140 | Classification loss: 0.27617 | Regression loss: 0.40714 | Running loss: 0.72950
2020-06-26 13:08:39,966 - retinanet_logs - INFO - Epoch: 4 | Iteration: 160 | Classification loss: 0.37671 | Regression loss: 0.40488 | Running loss: 0.72218
2020-06-26 13:08:51,801 - retinanet_logs - INFO - Epoch: 4 | Iteration: 180 | Classification loss: 0.21798 | Regression loss: 0.27964 | Running loss: 0.71173
2020-06-26 13:09:03,177 - retinanet_logs - INFO - Epoch: 4 | Iteration: 200 | Classification loss: 0.20233 | Regression loss: 0.23111 | Running loss: 0.70451
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.9588235294117647
rose: 0.6977272727272728
sunflower: 1.0
lotus: 0.45
carnation: 0.5714285714285714
daffodill: 0.9733333333333334
magnolia: 0.25
camellia: 0.8454545454545455
dandelion: 0.6834523809523809
daisy: 1.0
mugungwha: 0.5073863636363636
amalea: 0.23316326530612247
moran: 0.5108333333333334
cosmos: 0.3111658249158249
lily: 0.3376519916142558
pasqueflower: 0.6258504128069344
morning glory: 0.7050000000000001
wild rose: 0.6996392496392496
hydrangea: 0.3334779367918903
droplet: 0.7094017094017094
freesia: 0.4502673796791444
2020-06-26 13:09:14,103 - retinanet_logs - INFO - Average accuracy : 0.61
2020-06-26 13:09:14,103 - retinanet_logs - INFO - Average loss 0.65:
2020-06-26 13:09:15,391 - retinanet_logs - INFO - Epoch: 5 | Iteration: 0 | Classification loss: 0.33941 | Regression loss: 0.33078 | Running loss: 0.70425
2020-06-26 13:09:26,761 - retinanet_logs - INFO - Epoch: 5 | Iteration: 20 | Classification loss: 0.14823 | Regression loss: 0.23896 | Running loss: 0.69496
2020-06-26 13:09:38,898 - retinanet_logs - INFO - Epoch: 5 | Iteration: 40 | Classification loss: 0.23744 | Regression loss: 0.29590 | Running loss: 0.68857
2020-06-26 13:09:50,878 - retinanet_logs - INFO - Epoch: 5 | Iteration: 60 | Classification loss: 0.30028 | Regression loss: 0.35718 | Running loss: 0.67905
2020-06-26 13:10:02,563 - retinanet_logs - INFO - Epoch: 5 | Iteration: 80 | Classification loss: 0.44437 | Regression loss: 0.41568 | Running loss: 0.67374
2020-06-26 13:10:14,776 - retinanet_logs - INFO - Epoch: 5 | Iteration: 100 | Classification loss: 0.31751 | Regression loss: 0.36867 | Running loss: 0.66697
2020-06-26 13:10:27,050 - retinanet_logs - INFO - Epoch: 5 | Iteration: 120 | Classification loss: 0.15944 | Regression loss: 0.24617 | Running loss: 0.66096
2020-06-26 13:10:38,886 - retinanet_logs - INFO - Epoch: 5 | Iteration: 140 | Classification loss: 0.18952 | Regression loss: 0.34206 | Running loss: 0.65667
2020-06-26 13:10:50,351 - retinanet_logs - INFO - Epoch: 5 | Iteration: 160 | Classification loss: 0.27498 | Regression loss: 0.34639 | Running loss: 0.65193
2020-06-26 13:11:01,707 - retinanet_logs - INFO - Epoch: 5 | Iteration: 180 | Classification loss: 0.20669 | Regression loss: 0.26890 | Running loss: 0.64175
2020-06-26 13:11:13,526 - retinanet_logs - INFO - Epoch: 5 | Iteration: 200 | Classification loss: 0.20409 | Regression loss: 0.39163 | Running loss: 0.63927
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.5583333333333333
sunflower: 1.0
lotus: 0.575
carnation: 0.788888888888889
daffodill: 0.89
magnolia: 0.325895875591616
camellia: 0.8454545454545455
dandelion: 0.6318181818181818
daisy: 1.0
mugungwha: 0.7505050505050506
amalea: 0.5859848484848484
moran: 0.4766666666666667
cosmos: 0.3983040935672515
lily: 0.43862781954887214
pasqueflower: 0.5
morning glory: 0.6925
wild rose: 0.7999125874125873
hydrangea: 0.4053107118689459
droplet: 0.6897435897435897
freesia: 0.6671768707482993
2020-06-26 13:11:24,411 - retinanet_logs - INFO - Average accuracy : 0.67
2020-06-26 13:11:24,412 - retinanet_logs - INFO - Average loss 0.59:
2020-06-26 13:11:25,575 - retinanet_logs - INFO - Epoch: 6 | Iteration: 0 | Classification loss: 0.29118 | Regression loss: 0.44555 | Running loss: 0.63843
2020-06-26 13:11:37,036 - retinanet_logs - INFO - Epoch: 6 | Iteration: 20 | Classification loss: 0.17719 | Regression loss: 0.36812 | Running loss: 0.63187
2020-06-26 13:11:49,024 - retinanet_logs - INFO - Epoch: 6 | Iteration: 40 | Classification loss: 0.14802 | Regression loss: 0.23903 | Running loss: 0.62261
2020-06-26 13:12:00,878 - retinanet_logs - INFO - Epoch: 6 | Iteration: 60 | Classification loss: 0.10446 | Regression loss: 0.22855 | Running loss: 0.61603
2020-06-26 13:12:12,208 - retinanet_logs - INFO - Epoch: 6 | Iteration: 80 | Classification loss: 0.22972 | Regression loss: 0.30150 | Running loss: 0.61281
2020-06-26 13:12:23,922 - retinanet_logs - INFO - Epoch: 6 | Iteration: 100 | Classification loss: 0.25111 | Regression loss: 0.51962 | Running loss: 0.60895
2020-06-26 13:12:35,878 - retinanet_logs - INFO - Epoch: 6 | Iteration: 120 | Classification loss: 0.25935 | Regression loss: 0.38659 | Running loss: 0.60471
2020-06-26 13:12:47,902 - retinanet_logs - INFO - Epoch: 6 | Iteration: 140 | Classification loss: 0.35570 | Regression loss: 0.50236 | Running loss: 0.59956
2020-06-26 13:12:59,310 - retinanet_logs - INFO - Epoch: 6 | Iteration: 160 | Classification loss: 0.09431 | Regression loss: 0.25956 | Running loss: 0.59353
2020-06-26 13:13:11,084 - retinanet_logs - INFO - Epoch: 6 | Iteration: 180 | Classification loss: 0.31751 | Regression loss: 0.49049 | Running loss: 0.58785
2020-06-26 13:13:22,887 - retinanet_logs - INFO - Epoch: 6 | Iteration: 200 | Classification loss: 0.14287 | Regression loss: 0.30152 | Running loss: 0.58149
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.7
sunflower: 1.0
lotus: 0.8
carnation: 0.824888888888889
daffodill: 0.9833333333333334
magnolia: 0.41785714285714287
camellia: 0.7925641025641026
dandelion: 0.6101465201465202
daisy: 1.0
mugungwha: 0.6877882205513784
amalea: 0.6804761904761905
moran: 0.25
cosmos: 0.48444444444444446
lily: 0.4736548269581057
pasqueflower: 0.4717948717948718
morning glory: 0.7041666666666666
wild rose: 0.8286435786435785
hydrangea: 0.4949630541871921
droplet: 0.7142857142857142
freesia: 0.6796296296296296
2020-06-26 13:13:33,897 - retinanet_logs - INFO - Average accuracy : 0.69
2020-06-26 13:13:33,897 - retinanet_logs - INFO - Average loss 0.55:
2020-06-26 13:13:34,821 - retinanet_logs - INFO - Epoch: 7 | Iteration: 0 | Classification loss: 0.13390 | Regression loss: 0.24015 | Running loss: 0.58014
2020-06-26 13:13:46,261 - retinanet_logs - INFO - Epoch: 7 | Iteration: 20 | Classification loss: 0.17062 | Regression loss: 0.35131 | Running loss: 0.57474
2020-06-26 13:13:58,295 - retinanet_logs - INFO - Epoch: 7 | Iteration: 40 | Classification loss: 0.20240 | Regression loss: 0.30301 | Running loss: 0.56980
2020-06-26 13:14:10,078 - retinanet_logs - INFO - Epoch: 7 | Iteration: 60 | Classification loss: 0.16147 | Regression loss: 0.32631 | Running loss: 0.56442
2020-06-26 13:14:21,952 - retinanet_logs - INFO - Epoch: 7 | Iteration: 80 | Classification loss: 0.21813 | Regression loss: 0.41308 | Running loss: 0.55690
2020-06-26 13:14:33,716 - retinanet_logs - INFO - Epoch: 7 | Iteration: 100 | Classification loss: 0.27081 | Regression loss: 0.38318 | Running loss: 0.55308
2020-06-26 13:14:45,664 - retinanet_logs - INFO - Epoch: 7 | Iteration: 120 | Classification loss: 0.35615 | Regression loss: 0.43389 | Running loss: 0.55123
2020-06-26 13:14:57,482 - retinanet_logs - INFO - Epoch: 7 | Iteration: 140 | Classification loss: 0.11763 | Regression loss: 0.15464 | Running loss: 0.54761
2020-06-26 13:15:09,907 - retinanet_logs - INFO - Epoch: 7 | Iteration: 160 | Classification loss: 0.23075 | Regression loss: 0.33670 | Running loss: 0.54595
2020-06-26 13:15:21,625 - retinanet_logs - INFO - Epoch: 7 | Iteration: 180 | Classification loss: 0.18118 | Regression loss: 0.31569 | Running loss: 0.54345
2020-06-26 13:15:32,778 - retinanet_logs - INFO - Epoch: 7 | Iteration: 200 | Classification loss: 0.14073 | Regression loss: 0.25608 | Running loss: 0.53778
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.9818181818181817
rose: 0.7
sunflower: 1.0
lotus: 0.89
carnation: 0.8562500000000001
daffodill: 0.9555555555555556
magnolia: 0.7779020979020979
camellia: 0.6749999999999999
dandelion: 0.6326050420168067
daisy: 1.0
mugungwha: 0.6555555555555556
amalea: 0.7027731092436974
moran: 0.4666666666666667
cosmos: 0.6783612040133781
lily: 0.5454722250345921
pasqueflower: 0.5082191780821917
morning glory: 0.7784722222222222
wild rose: 0.8234782608695652
hydrangea: 0.43478260869565216
droplet: 0.6916666666666667
freesia: 0.5583333333333333
2020-06-26 13:15:43,414 - retinanet_logs - INFO - Average accuracy : 0.73
2020-06-26 13:15:43,414 - retinanet_logs - INFO - Average loss 0.51:
2020-06-26 13:15:44,686 - retinanet_logs - INFO - Epoch: 8 | Iteration: 0 | Classification loss: 0.13786 | Regression loss: 0.28643 | Running loss: 0.53590
2020-06-26 13:15:56,324 - retinanet_logs - INFO - Epoch: 8 | Iteration: 20 | Classification loss: 0.14562 | Regression loss: 0.35201 | Running loss: 0.53346
2020-06-26 13:16:08,121 - retinanet_logs - INFO - Epoch: 8 | Iteration: 40 | Classification loss: 0.08373 | Regression loss: 0.18754 | Running loss: 0.52757
2020-06-26 13:16:19,276 - retinanet_logs - INFO - Epoch: 8 | Iteration: 60 | Classification loss: 0.15824 | Regression loss: 0.25939 | Running loss: 0.52579
2020-06-26 13:16:31,430 - retinanet_logs - INFO - Epoch: 8 | Iteration: 80 | Classification loss: 0.28495 | Regression loss: 0.31614 | Running loss: 0.52237
2020-06-26 13:16:42,801 - retinanet_logs - INFO - Epoch: 8 | Iteration: 100 | Classification loss: 0.16677 | Regression loss: 0.38679 | Running loss: 0.51912
2020-06-26 13:16:55,109 - retinanet_logs - INFO - Epoch: 8 | Iteration: 120 | Classification loss: 0.14976 | Regression loss: 0.31669 | Running loss: 0.51707
2020-06-26 13:17:07,174 - retinanet_logs - INFO - Epoch: 8 | Iteration: 140 | Classification loss: 0.16450 | Regression loss: 0.25020 | Running loss: 0.51463
2020-06-26 13:17:18,922 - retinanet_logs - INFO - Epoch: 8 | Iteration: 160 | Classification loss: 0.14355 | Regression loss: 0.36083 | Running loss: 0.51070
2020-06-26 13:17:30,735 - retinanet_logs - INFO - Epoch: 8 | Iteration: 180 | Classification loss: 0.16084 | Regression loss: 0.27685 | Running loss: 0.50819
2020-06-26 13:17:42,072 - retinanet_logs - INFO - Epoch: 8 | Iteration: 200 | Classification loss: 0.11895 | Regression loss: 0.30829 | Running loss: 0.50074
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.8942612942612942
sunflower: 1.0
lotus: 0.6
carnation: 0.78
daffodill: 0.9833333333333334
magnolia: 0.78432301438399
camellia: 0.8484848484848484
dandelion: 0.6945054945054945
daisy: 1.0
mugungwha: 0.721933621933622
amalea: 0.709666447764943
moran: 0.6699999999999999
cosmos: 0.5266414141414142
lily: 0.4691252587991719
pasqueflower: 0.5
morning glory: 0.7050000000000001
wild rose: 0.8699999999999999
hydrangea: 0.4204811253561254
droplet: 0.8012820512820513
freesia: 0.8386363636363636
2020-06-26 13:17:52,951 - retinanet_logs - INFO - Average accuracy : 0.75
2020-06-26 13:17:52,951 - retinanet_logs - INFO - Average loss 0.48:
2020-06-26 13:17:54,070 - retinanet_logs - INFO - Epoch: 9 | Iteration: 0 | Classification loss: 0.11607 | Regression loss: 0.15019 | Running loss: 0.49962
2020-06-26 13:18:05,900 - retinanet_logs - INFO - Epoch: 9 | Iteration: 20 | Classification loss: 0.20279 | Regression loss: 0.44916 | Running loss: 0.49845
2020-06-26 13:18:17,508 - retinanet_logs - INFO - Epoch: 9 | Iteration: 40 | Classification loss: 0.06173 | Regression loss: 0.19239 | Running loss: 0.49079
2020-06-26 13:18:29,154 - retinanet_logs - INFO - Epoch: 9 | Iteration: 60 | Classification loss: 0.08293 | Regression loss: 0.16207 | Running loss: 0.48877
2020-06-26 13:18:40,978 - retinanet_logs - INFO - Epoch: 9 | Iteration: 80 | Classification loss: 0.20182 | Regression loss: 0.34405 | Running loss: 0.48566
2020-06-26 13:18:52,947 - retinanet_logs - INFO - Epoch: 9 | Iteration: 100 | Classification loss: 0.16274 | Regression loss: 0.25239 | Running loss: 0.48324
2020-06-26 13:19:04,625 - retinanet_logs - INFO - Epoch: 9 | Iteration: 120 | Classification loss: 0.14708 | Regression loss: 0.28865 | Running loss: 0.48322
2020-06-26 13:19:16,625 - retinanet_logs - INFO - Epoch: 9 | Iteration: 140 | Classification loss: 0.15341 | Regression loss: 0.23960 | Running loss: 0.47950
2020-06-26 13:19:28,393 - retinanet_logs - INFO - Epoch: 9 | Iteration: 160 | Classification loss: 0.15268 | Regression loss: 0.30092 | Running loss: 0.48001
2020-06-26 13:19:40,069 - retinanet_logs - INFO - Epoch: 9 | Iteration: 180 | Classification loss: 0.06569 | Regression loss: 0.14852 | Running loss: 0.47778
2020-06-26 13:19:51,547 - retinanet_logs - INFO - Epoch: 9 | Iteration: 200 | Classification loss: 0.12575 | Regression loss: 0.26079 | Running loss: 0.47486
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.9
rose: 0.5114285714285713
sunflower: 1.0
lotus: 0.8
carnation: 0.6875
daffodill: 0.9833333333333334
magnolia: 0.4079824561403509
camellia: 0.5
dandelion: 0.6493506493506493
daisy: 1.0
mugungwha: 0.5834599156118143
amalea: 0.6946758671570702
moran: 0.18
cosmos: 0.5728843086737824
lily: 0.5931601731601732
pasqueflower: 0.5
morning glory: 0.7222222222222222
wild rose: 0.8566719829877724
hydrangea: 0.47096252483711476
droplet: 0.7864285714285715
freesia: 0.7610640567162307
2020-06-26 13:20:02,449 - retinanet_logs - INFO - Average accuracy : 0.67
2020-06-26 13:20:02,450 - retinanet_logs - INFO - Average loss 0.45:
2020-06-26 13:20:03,598 - retinanet_logs - INFO - Epoch: 10 | Iteration: 0 | Classification loss: 0.13745 | Regression loss: 0.20248 | Running loss: 0.47383
2020-06-26 13:20:15,432 - retinanet_logs - INFO - Epoch: 10 | Iteration: 20 | Classification loss: 0.17159 | Regression loss: 0.28740 | Running loss: 0.46911
2020-06-26 13:20:27,054 - retinanet_logs - INFO - Epoch: 10 | Iteration: 40 | Classification loss: 0.14165 | Regression loss: 0.28220 | Running loss: 0.46587
2020-06-26 13:20:38,667 - retinanet_logs - INFO - Epoch: 10 | Iteration: 60 | Classification loss: 0.07440 | Regression loss: 0.16610 | Running loss: 0.46004
2020-06-26 13:20:50,705 - retinanet_logs - INFO - Epoch: 10 | Iteration: 80 | Classification loss: 0.07782 | Regression loss: 0.21925 | Running loss: 0.45648
2020-06-26 13:21:02,418 - retinanet_logs - INFO - Epoch: 10 | Iteration: 100 | Classification loss: 0.20825 | Regression loss: 0.32180 | Running loss: 0.45446
2020-06-26 13:21:14,150 - retinanet_logs - INFO - Epoch: 10 | Iteration: 120 | Classification loss: 0.14561 | Regression loss: 0.30342 | Running loss: 0.44962
2020-06-26 13:21:25,273 - retinanet_logs - INFO - Epoch: 10 | Iteration: 140 | Classification loss: 0.10147 | Regression loss: 0.20643 | Running loss: 0.44788
2020-06-26 13:21:37,254 - retinanet_logs - INFO - Epoch: 10 | Iteration: 160 | Classification loss: 0.22406 | Regression loss: 0.27518 | Running loss: 0.44706
2020-06-26 13:21:48,738 - retinanet_logs - INFO - Epoch: 10 | Iteration: 180 | Classification loss: 0.10767 | Regression loss: 0.31361 | Running loss: 0.44372
2020-06-26 13:22:01,104 - retinanet_logs - INFO - Epoch: 10 | Iteration: 200 | Classification loss: 0.14236 | Regression loss: 0.31888 | Running loss: 0.44079
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.9
rose: 0.86
sunflower: 1.0
lotus: 0.7723636363636364
carnation: 0.732
daffodill: 1.0
magnolia: 0.757
camellia: 0.755
dandelion: 0.6368421052631579
daisy: 1.0
mugungwha: 0.7555555555555555
amalea: 0.740021901600849
moran: 0.6555555555555556
cosmos: 0.5545774882394601
lily: 0.4802308802308803
pasqueflower: 0.5
morning glory: 0.6865384615384615
wild rose: 0.8516594516594516
hydrangea: 0.39297491723962313
droplet: 0.7247619047619047
freesia: 0.7035714285714285
2020-06-26 13:22:11,923 - retinanet_logs - INFO - Average accuracy : 0.74
2020-06-26 13:22:11,924 - retinanet_logs - INFO - Average loss 0.42:
2020-06-26 13:22:12,828 - retinanet_logs - INFO - Epoch: 11 | Iteration: 0 | Classification loss: 0.10030 | Regression loss: 0.23857 | Running loss: 0.43968
2020-06-26 13:22:24,073 - retinanet_logs - INFO - Epoch: 11 | Iteration: 20 | Classification loss: 0.07346 | Regression loss: 0.23977 | Running loss: 0.43496
2020-06-26 13:22:35,835 - retinanet_logs - INFO - Epoch: 11 | Iteration: 40 | Classification loss: 0.06804 | Regression loss: 0.17443 | Running loss: 0.43089
2020-06-26 13:22:47,265 - retinanet_logs - INFO - Epoch: 11 | Iteration: 60 | Classification loss: 0.13570 | Regression loss: 0.37439 | Running loss: 0.42467
2020-06-26 13:22:58,763 - retinanet_logs - INFO - Epoch: 11 | Iteration: 80 | Classification loss: 0.13793 | Regression loss: 0.25423 | Running loss: 0.42292
2020-06-26 13:23:10,765 - retinanet_logs - INFO - Epoch: 11 | Iteration: 100 | Classification loss: 0.16247 | Regression loss: 0.22976 | Running loss: 0.42083
2020-06-26 13:23:21,906 - retinanet_logs - INFO - Epoch: 11 | Iteration: 120 | Classification loss: 0.12913 | Regression loss: 0.27177 | Running loss: 0.41811
2020-06-26 13:23:33,587 - retinanet_logs - INFO - Epoch: 11 | Iteration: 140 | Classification loss: 0.17140 | Regression loss: 0.12044 | Running loss: 0.41760
2020-06-26 13:23:46,542 - retinanet_logs - INFO - Epoch: 11 | Iteration: 160 | Classification loss: 0.10149 | Regression loss: 0.25751 | Running loss: 0.41828
2020-06-26 13:23:58,464 - retinanet_logs - INFO - Epoch: 11 | Iteration: 180 | Classification loss: 0.21667 | Regression loss: 0.42265 | Running loss: 0.42076
2020-06-26 13:24:10,295 - retinanet_logs - INFO - Epoch: 11 | Iteration: 200 | Classification loss: 0.30051 | Regression loss: 0.42518 | Running loss: 0.41989
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.8821428571428571
sunflower: 1.0
lotus: 0.8
carnation: 0.788888888888889
daffodill: 0.990909090909091
magnolia: 0.5478194103194104
camellia: 0.8484848484848484
dandelion: 0.6699999999999999
daisy: 1.0
mugungwha: 0.6802901258020938
amalea: 0.7282051282051283
moran: 0.806888888888889
cosmos: 0.5826704545454545
lily: 0.45766045199178634
pasqueflower: 0.5
morning glory: 0.7222222222222222
wild rose: 0.8570370370370369
hydrangea: 0.6354700854700855
droplet: 0.7464285714285714
freesia: 0.6760683760683761
2020-06-26 13:24:21,229 - retinanet_logs - INFO - Average accuracy : 0.76
2020-06-26 13:24:21,229 - retinanet_logs - INFO - Average loss 0.40:
2020-06-26 13:24:22,263 - retinanet_logs - INFO - Epoch: 12 | Iteration: 0 | Classification loss: 0.11459 | Regression loss: 0.21716 | Running loss: 0.41933
2020-06-26 13:24:33,357 - retinanet_logs - INFO - Epoch: 12 | Iteration: 20 | Classification loss: 0.05148 | Regression loss: 0.15149 | Running loss: 0.41719
2020-06-26 13:24:44,527 - retinanet_logs - INFO - Epoch: 12 | Iteration: 40 | Classification loss: 0.06479 | Regression loss: 0.16638 | Running loss: 0.41507
2020-06-26 13:24:56,590 - retinanet_logs - INFO - Epoch: 12 | Iteration: 60 | Classification loss: 0.06518 | Regression loss: 0.13319 | Running loss: 0.40970
2020-06-26 13:25:08,557 - retinanet_logs - INFO - Epoch: 12 | Iteration: 80 | Classification loss: 0.26010 | Regression loss: 0.33641 | Running loss: 0.40677
2020-06-26 13:25:20,327 - retinanet_logs - INFO - Epoch: 12 | Iteration: 100 | Classification loss: 0.11709 | Regression loss: 0.19085 | Running loss: 0.40432
2020-06-26 13:25:32,429 - retinanet_logs - INFO - Epoch: 12 | Iteration: 120 | Classification loss: 0.14432 | Regression loss: 0.23661 | Running loss: 0.40250
2020-06-26 13:25:44,056 - retinanet_logs - INFO - Epoch: 12 | Iteration: 140 | Classification loss: 0.04624 | Regression loss: 0.15664 | Running loss: 0.39882
2020-06-26 13:25:56,298 - retinanet_logs - INFO - Epoch: 12 | Iteration: 160 | Classification loss: 0.06593 | Regression loss: 0.19252 | Running loss: 0.39804
2020-06-26 13:26:07,723 - retinanet_logs - INFO - Epoch: 12 | Iteration: 180 | Classification loss: 0.07110 | Regression loss: 0.16001 | Running loss: 0.39795
2020-06-26 13:26:19,620 - retinanet_logs - INFO - Epoch: 12 | Iteration: 200 | Classification loss: 0.16001 | Regression loss: 0.28796 | Running loss: 0.39733
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.9714285714285714
sunflower: 1.0
lotus: 0.7
carnation: 0.8333333333333334
daffodill: 1.0
magnolia: 0.7942857142857143
camellia: 0.7
dandelion: 0.6291666666666667
daisy: 1.0
mugungwha: 0.7282828282828283
amalea: 0.696031746031746
moran: 0.915
cosmos: 0.6057919666540356
lily: 0.5719327731092437
pasqueflower: 0.6269298245614034
morning glory: 0.711111111111111
wild rose: 0.9033333333333333
hydrangea: 0.5925318761384335
droplet: 0.8012820512820513
freesia: 0.7393162393162394
2020-06-26 13:26:30,428 - retinanet_logs - INFO - Average accuracy : 0.79
2020-06-26 13:26:30,428 - retinanet_logs - INFO - Average loss 0.38:
2020-06-26 13:26:31,602 - retinanet_logs - INFO - Epoch: 13 | Iteration: 0 | Classification loss: 0.17258 | Regression loss: 0.30721 | Running loss: 0.39811
2020-06-26 13:26:42,765 - retinanet_logs - INFO - Epoch: 13 | Iteration: 20 | Classification loss: 0.08686 | Regression loss: 0.23101 | Running loss: 0.39575
2020-06-26 13:26:54,387 - retinanet_logs - INFO - Epoch: 13 | Iteration: 40 | Classification loss: 0.04952 | Regression loss: 0.19112 | Running loss: 0.39314
2020-06-26 13:27:06,435 - retinanet_logs - INFO - Epoch: 13 | Iteration: 60 | Classification loss: 0.08580 | Regression loss: 0.28891 | Running loss: 0.39012
2020-06-26 13:27:18,237 - retinanet_logs - INFO - Epoch: 13 | Iteration: 80 | Classification loss: 0.10179 | Regression loss: 0.28279 | Running loss: 0.38456
2020-06-26 13:27:29,707 - retinanet_logs - INFO - Epoch: 13 | Iteration: 100 | Classification loss: 0.13611 | Regression loss: 0.30271 | Running loss: 0.38498
2020-06-26 13:27:41,345 - retinanet_logs - INFO - Epoch: 13 | Iteration: 120 | Classification loss: 0.25037 | Regression loss: 0.26319 | Running loss: 0.38585
2020-06-26 13:27:53,185 - retinanet_logs - INFO - Epoch: 13 | Iteration: 140 | Classification loss: 0.18020 | Regression loss: 0.28305 | Running loss: 0.38743
2020-06-26 13:28:04,289 - retinanet_logs - INFO - Epoch: 13 | Iteration: 160 | Classification loss: 0.07426 | Regression loss: 0.20862 | Running loss: 0.38507
2020-06-26 13:28:16,481 - retinanet_logs - INFO - Epoch: 13 | Iteration: 180 | Classification loss: 0.14166 | Regression loss: 0.23855 | Running loss: 0.38408
2020-06-26 13:28:28,692 - retinanet_logs - INFO - Epoch: 13 | Iteration: 200 | Classification loss: 0.15688 | Regression loss: 0.26402 | Running loss: 0.38375
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.89
sunflower: 1.0
lotus: 0.8
carnation: 0.9
daffodill: 0.990909090909091
magnolia: 0.6619047619047619
camellia: 0.8484848484848484
dandelion: 0.7366666666666667
daisy: 1.0
mugungwha: 0.8207142857142857
amalea: 0.7047619047619047
moran: 0.7
cosmos: 0.6121794871794871
lily: 0.5994089945560533
pasqueflower: 0.5850877192982455
morning glory: 0.711111111111111
wild rose: 0.8584615384615384
hydrangea: 0.673578431372549
droplet: 0.725
freesia: 0.6222222222222222
2020-06-26 13:28:39,307 - retinanet_logs - INFO - Average accuracy : 0.78
2020-06-26 13:28:39,307 - retinanet_logs - INFO - Average loss 0.36:
2020-06-26 13:28:40,543 - retinanet_logs - INFO - Epoch: 14 | Iteration: 0 | Classification loss: 0.11350 | Regression loss: 0.18559 | Running loss: 0.38304
2020-06-26 13:28:52,297 - retinanet_logs - INFO - Epoch: 14 | Iteration: 20 | Classification loss: 0.09239 | Regression loss: 0.17292 | Running loss: 0.37845
2020-06-26 13:29:03,710 - retinanet_logs - INFO - Epoch: 14 | Iteration: 40 | Classification loss: 0.17600 | Regression loss: 0.30273 | Running loss: 0.37405
2020-06-26 13:29:15,722 - retinanet_logs - INFO - Epoch: 14 | Iteration: 60 | Classification loss: 0.06306 | Regression loss: 0.12553 | Running loss: 0.36807
2020-06-26 13:29:27,006 - retinanet_logs - INFO - Epoch: 14 | Iteration: 80 | Classification loss: 0.15414 | Regression loss: 0.31710 | Running loss: 0.36437
2020-06-26 13:29:38,785 - retinanet_logs - INFO - Epoch: 14 | Iteration: 100 | Classification loss: 0.13896 | Regression loss: 0.34017 | Running loss: 0.36087
2020-06-26 13:29:51,497 - retinanet_logs - INFO - Epoch: 14 | Iteration: 120 | Classification loss: 0.10099 | Regression loss: 0.26574 | Running loss: 0.35893
2020-06-26 13:30:02,956 - retinanet_logs - INFO - Epoch: 14 | Iteration: 140 | Classification loss: 0.14828 | Regression loss: 0.34201 | Running loss: 0.35853
2020-06-26 13:30:14,586 - retinanet_logs - INFO - Epoch: 14 | Iteration: 160 | Classification loss: 0.19071 | Regression loss: 0.39517 | Running loss: 0.35763
2020-06-26 13:30:26,292 - retinanet_logs - INFO - Epoch: 14 | Iteration: 180 | Classification loss: 0.11490 | Regression loss: 0.21116 | Running loss: 0.35817
2020-06-26 13:30:37,774 - retinanet_logs - INFO - Epoch: 14 | Iteration: 200 | Classification loss: 0.03686 | Regression loss: 0.17285 | Running loss: 0.35730
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.9669230769230769
sunflower: 1.0
lotus: 0.9
carnation: 0.8
daffodill: 1.0
magnolia: 0.6262421233963462
camellia: 0.86
dandelion: 0.635
daisy: 1.0
mugungwha: 0.7777777777777778
amalea: 0.792063492063492
moran: 0.7380952380952381
cosmos: 0.35140424590888986
lily: 0.46595109568793774
pasqueflower: 0.679531152485958
morning glory: 0.711111111111111
wild rose: 0.7612500000000001
hydrangea: 0.536053391053391
droplet: 0.7815384615384615
freesia: 0.7000000000000001
2020-06-26 13:30:48,545 - retinanet_logs - INFO - Average accuracy : 0.77
2020-06-26 13:30:48,545 - retinanet_logs - INFO - Average loss 0.34:
2020-06-26 13:30:49,722 - retinanet_logs - INFO - Epoch: 15 | Iteration: 0 | Classification loss: 0.15222 | Regression loss: 0.25053 | Running loss: 0.35722
2020-06-26 13:31:01,000 - retinanet_logs - INFO - Epoch: 15 | Iteration: 20 | Classification loss: 0.11494 | Regression loss: 0.26419 | Running loss: 0.35463
2020-06-26 13:31:12,724 - retinanet_logs - INFO - Epoch: 15 | Iteration: 40 | Classification loss: 0.10110 | Regression loss: 0.23613 | Running loss: 0.35164
2020-06-26 13:31:24,178 - retinanet_logs - INFO - Epoch: 15 | Iteration: 60 | Classification loss: 0.23848 | Regression loss: 0.30200 | Running loss: 0.35260
2020-06-26 13:31:36,086 - retinanet_logs - INFO - Epoch: 15 | Iteration: 80 | Classification loss: 0.03834 | Regression loss: 0.19635 | Running loss: 0.34885
2020-06-26 13:31:47,731 - retinanet_logs - INFO - Epoch: 15 | Iteration: 100 | Classification loss: 0.10234 | Regression loss: 0.24824 | Running loss: 0.34717
2020-06-26 13:31:59,348 - retinanet_logs - INFO - Epoch: 15 | Iteration: 120 | Classification loss: 0.09020 | Regression loss: 0.21960 | Running loss: 0.34704
2020-06-26 13:32:11,205 - retinanet_logs - INFO - Epoch: 15 | Iteration: 140 | Classification loss: 0.10140 | Regression loss: 0.24413 | Running loss: 0.34577
2020-06-26 13:32:22,989 - retinanet_logs - INFO - Epoch: 15 | Iteration: 160 | Classification loss: 0.15628 | Regression loss: 0.35648 | Running loss: 0.34689
2020-06-26 13:32:34,646 - retinanet_logs - INFO - Epoch: 15 | Iteration: 180 | Classification loss: 0.06885 | Regression loss: 0.17959 | Running loss: 0.34567
2020-06-26 13:32:46,899 - retinanet_logs - INFO - Epoch: 15 | Iteration: 200 | Classification loss: 0.14605 | Regression loss: 0.29486 | Running loss: 0.34482
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.89
sunflower: 1.0
lotus: 0.9
carnation: 0.732
daffodill: 1.0
magnolia: 0.8
camellia: 0.8638888888888889
dandelion: 0.6777777777777778
daisy: 1.0
mugungwha: 0.7444444444444445
amalea: 0.7187987012987013
moran: 0.788888888888889
cosmos: 0.5952348690153568
lily: 0.5255874316939891
pasqueflower: 0.5485507246376812
morning glory: 0.7222222222222222
wild rose: 0.8788235294117647
hydrangea: 0.3760575167804084
droplet: 0.7752136752136751
freesia: 0.6821733821733822
2020-06-26 13:32:57,679 - retinanet_logs - INFO - Average accuracy : 0.77
2020-06-26 13:32:57,679 - retinanet_logs - INFO - Average loss 0.34:
2020-06-26 13:32:58,782 - retinanet_logs - INFO - Epoch: 16 | Iteration: 0 | Classification loss: 0.18272 | Regression loss: 0.36270 | Running loss: 0.34576
2020-06-26 13:33:10,271 - retinanet_logs - INFO - Epoch: 16 | Iteration: 20 | Classification loss: 0.10763 | Regression loss: 0.22571 | Running loss: 0.34335
2020-06-26 13:33:21,980 - retinanet_logs - INFO - Epoch: 16 | Iteration: 40 | Classification loss: 0.03338 | Regression loss: 0.14604 | Running loss: 0.34095
2020-06-26 13:33:33,468 - retinanet_logs - INFO - Epoch: 16 | Iteration: 60 | Classification loss: 0.09245 | Regression loss: 0.23946 | Running loss: 0.33735
2020-06-26 13:33:45,333 - retinanet_logs - INFO - Epoch: 16 | Iteration: 80 | Classification loss: 0.03890 | Regression loss: 0.14438 | Running loss: 0.33605
2020-06-26 13:33:56,981 - retinanet_logs - INFO - Epoch: 16 | Iteration: 100 | Classification loss: 0.02084 | Regression loss: 0.10402 | Running loss: 0.33572
2020-06-26 13:34:08,603 - retinanet_logs - INFO - Epoch: 16 | Iteration: 120 | Classification loss: 0.13355 | Regression loss: 0.32531 | Running loss: 0.33617
2020-06-26 13:34:20,336 - retinanet_logs - INFO - Epoch: 16 | Iteration: 140 | Classification loss: 0.13684 | Regression loss: 0.27512 | Running loss: 0.33541
2020-06-26 13:34:31,834 - retinanet_logs - INFO - Epoch: 16 | Iteration: 160 | Classification loss: 0.05451 | Regression loss: 0.15221 | Running loss: 0.33431
2020-06-26 13:34:43,447 - retinanet_logs - INFO - Epoch: 16 | Iteration: 180 | Classification loss: 0.02902 | Regression loss: 0.10402 | Running loss: 0.33403
2020-06-26 13:34:55,214 - retinanet_logs - INFO - Epoch: 16 | Iteration: 200 | Classification loss: 0.13100 | Regression loss: 0.23936 | Running loss: 0.33477
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.95
sunflower: 1.0
lotus: 0.8
carnation: 0.9
daffodill: 1.0
magnolia: 0.8706349206349207
camellia: 0.8
dandelion: 0.7333333333333334
daisy: 1.0
mugungwha: 0.8168831168831169
amalea: 0.685064935064935
moran: 0.85
cosmos: 0.7673013923013923
lily: 0.6401709401709402
pasqueflower: 0.6
morning glory: 0.7444444444444445
wild rose: 0.8676190476190475
hydrangea: 0.6147581585081585
droplet: 0.8314685314685314
freesia: 0.8442857142857143
2020-06-26 13:35:06,631 - retinanet_logs - INFO - Average accuracy : 0.82
2020-06-26 13:35:06,631 - retinanet_logs - INFO - Average loss 0.32:
2020-06-26 13:35:07,737 - retinanet_logs - INFO - Epoch: 17 | Iteration: 0 | Classification loss: 0.08905 | Regression loss: 0.19497 | Running loss: 0.33359
2020-06-26 13:35:18,810 - retinanet_logs - INFO - Epoch: 17 | Iteration: 20 | Classification loss: 0.06920 | Regression loss: 0.15275 | Running loss: 0.33240
2020-06-26 13:35:30,581 - retinanet_logs - INFO - Epoch: 17 | Iteration: 40 | Classification loss: 0.13688 | Regression loss: 0.21300 | Running loss: 0.33007
2020-06-26 13:35:42,362 - retinanet_logs - INFO - Epoch: 17 | Iteration: 60 | Classification loss: 0.07091 | Regression loss: 0.21755 | Running loss: 0.32800
2020-06-26 13:35:54,528 - retinanet_logs - INFO - Epoch: 17 | Iteration: 80 | Classification loss: 0.09993 | Regression loss: 0.24791 | Running loss: 0.32452
2020-06-26 13:36:06,294 - retinanet_logs - INFO - Epoch: 17 | Iteration: 100 | Classification loss: 0.09994 | Regression loss: 0.26564 | Running loss: 0.32374
2020-06-26 13:36:17,661 - retinanet_logs - INFO - Epoch: 17 | Iteration: 120 | Classification loss: 0.17316 | Regression loss: 0.25693 | Running loss: 0.32405
2020-06-26 13:36:29,509 - retinanet_logs - INFO - Epoch: 17 | Iteration: 140 | Classification loss: 0.08342 | Regression loss: 0.27028 | Running loss: 0.32060
2020-06-26 13:36:41,409 - retinanet_logs - INFO - Epoch: 17 | Iteration: 160 | Classification loss: 0.06480 | Regression loss: 0.14699 | Running loss: 0.31942
2020-06-26 13:36:52,823 - retinanet_logs - INFO - Epoch: 17 | Iteration: 180 | Classification loss: 0.05370 | Regression loss: 0.16568 | Running loss: 0.31652
2020-06-26 13:37:04,673 - retinanet_logs - INFO - Epoch: 17 | Iteration: 200 | Classification loss: 0.16963 | Regression loss: 0.33162 | Running loss: 0.31654
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 1.0
rose: 0.86
sunflower: 1.0
lotus: 0.9
carnation: 0.9406417112299466
daffodill: 0.9588235294117647
magnolia: 0.8167252886002885
camellia: 0.788888888888889
dandelion: 0.7230769230769232
daisy: 1.0
mugungwha: 0.76
amalea: 0.7126872345182204
moran: 0.8
cosmos: 0.6195652173913043
lily: 0.46745512820512825
pasqueflower: 0.6581485587583149
morning glory: 0.625
wild rose: 0.8755555555555555
hydrangea: 0.6175739957716703
droplet: 0.8077922077922077
freesia: 0.8083333333333332
2020-06-26 13:37:15,373 - retinanet_logs - INFO - Average accuracy : 0.80
2020-06-26 13:37:15,373 - retinanet_logs - INFO - Average loss 0.30:
2020-06-26 13:37:16,650 - retinanet_logs - INFO - Epoch: 18 | Iteration: 0 | Classification loss: 0.08065 | Regression loss: 0.17875 | Running loss: 0.31670
2020-06-26 13:37:28,188 - retinanet_logs - INFO - Epoch: 18 | Iteration: 20 | Classification loss: 0.09771 | Regression loss: 0.24442 | Running loss: 0.31478
2020-06-26 13:37:40,229 - retinanet_logs - INFO - Epoch: 18 | Iteration: 40 | Classification loss: 0.13170 | Regression loss: 0.35983 | Running loss: 0.31254
2020-06-26 13:37:52,132 - retinanet_logs - INFO - Epoch: 18 | Iteration: 60 | Classification loss: 0.07647 | Regression loss: 0.22738 | Running loss: 0.31044
2020-06-26 13:38:03,266 - retinanet_logs - INFO - Epoch: 18 | Iteration: 80 | Classification loss: 0.10738 | Regression loss: 0.26653 | Running loss: 0.30883
2020-06-26 13:38:14,794 - retinanet_logs - INFO - Epoch: 18 | Iteration: 100 | Classification loss: 0.05451 | Regression loss: 0.14880 | Running loss: 0.30480
2020-06-26 13:38:26,763 - retinanet_logs - INFO - Epoch: 18 | Iteration: 120 | Classification loss: 0.08940 | Regression loss: 0.25731 | Running loss: 0.30525
2020-06-26 13:38:38,611 - retinanet_logs - INFO - Epoch: 18 | Iteration: 140 | Classification loss: 0.06617 | Regression loss: 0.16013 | Running loss: 0.30523
2020-06-26 13:38:49,962 - retinanet_logs - INFO - Epoch: 18 | Iteration: 160 | Classification loss: 0.16886 | Regression loss: 0.33365 | Running loss: 0.30524
2020-06-26 13:39:01,609 - retinanet_logs - INFO - Epoch: 18 | Iteration: 180 | Classification loss: 0.04354 | Regression loss: 0.16085 | Running loss: 0.30478
2020-06-26 13:39:13,670 - retinanet_logs - INFO - Epoch: 18 | Iteration: 200 | Classification loss: 0.11832 | Regression loss: 0.21635 | Running loss: 0.30321
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 1.0
rose: 1.0
sunflower: 1.0
lotus: 0.8
carnation: 0.9
daffodill: 0.990909090909091
magnolia: 0.7833041958041959
camellia: 0.7666666666666666
dandelion: 0.7333333333333334
daisy: 1.0
mugungwha: 0.9094017094017093
amalea: 0.7616279069767441
moran: 0.6875
cosmos: 0.6662225705329153
lily: 0.49977106227106227
pasqueflower: 0.5
morning glory: 0.7333333333333333
wild rose: 0.8866666666666666
hydrangea: 0.5146153846153847
droplet: 0.7464285714285714
freesia: 0.83
2020-06-26 13:39:24,378 - retinanet_logs - INFO - Average accuracy : 0.80
2020-06-26 13:39:24,378 - retinanet_logs - INFO - Average loss 0.29:
2020-06-26 13:39:25,506 - retinanet_logs - INFO - Epoch: 19 | Iteration: 0 | Classification loss: 0.01663 | Regression loss: 0.12790 | Running loss: 0.30299
2020-06-26 13:39:37,218 - retinanet_logs - INFO - Epoch: 19 | Iteration: 20 | Classification loss: 0.06291 | Regression loss: 0.20823 | Running loss: 0.29904
2020-06-26 13:39:48,521 - retinanet_logs - INFO - Epoch: 19 | Iteration: 40 | Classification loss: 0.04039 | Regression loss: 0.15608 | Running loss: 0.29733
2020-06-26 13:40:00,405 - retinanet_logs - INFO - Epoch: 19 | Iteration: 60 | Classification loss: 0.09421 | Regression loss: 0.24045 | Running loss: 0.29693
2020-06-26 13:40:11,974 - retinanet_logs - INFO - Epoch: 19 | Iteration: 80 | Classification loss: 0.02618 | Regression loss: 0.12160 | Running loss: 0.29453
2020-06-26 13:40:23,502 - retinanet_logs - INFO - Epoch: 19 | Iteration: 100 | Classification loss: 0.10742 | Regression loss: 0.27309 | Running loss: 0.29328
2020-06-26 13:40:35,606 - retinanet_logs - INFO - Epoch: 19 | Iteration: 120 | Classification loss: 0.04516 | Regression loss: 0.20709 | Running loss: 0.29536
2020-06-26 13:40:47,106 - retinanet_logs - INFO - Epoch: 19 | Iteration: 140 | Classification loss: 0.06282 | Regression loss: 0.14195 | Running loss: 0.29254
2020-06-26 13:40:58,519 - retinanet_logs - INFO - Epoch: 19 | Iteration: 160 | Classification loss: 0.13261 | Regression loss: 0.30135 | Running loss: 0.28981
2020-06-26 13:41:10,285 - retinanet_logs - INFO - Epoch: 19 | Iteration: 180 | Classification loss: 0.08530 | Regression loss: 0.24126 | Running loss: 0.29017
2020-06-26 13:41:22,265 - retinanet_logs - INFO - Epoch: 19 | Iteration: 200 | Classification loss: 0.09508 | Regression loss: 0.30219 | Running loss: 0.29013
Evaluating dataset
1/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/1311/1312/1313/1314/1315/1316/1317/1318/1319/13110/13111/13112/13113/13114/13115/13116/13117/13118/13119/13120/13121/13122/13123/13124/13125/13126/13127/13128/13129/13130/13131/13132/13133/13134/13135/13136/13137/13138/13139/13140/13141/13142/13143/13144/13145/13146/13147/13148/13149/13150/13151/13152/13153/13154/13155/13156/13157/13158/13159/13160/13161/13162/13163/13164/13165/13166/13167/13168/13169/13170/13171/13172/13173/13174/13175/13176/13177/13178/13179/13180/13181/13182/13183/13184/13185/13186/13187/13188/13189/13190/13191/13192/13193/13194/13195/13196/13197/13198/13199/131100/131101/131102/131103/131104/131105/131106/131107/131108/131109/131110/131111/131112/131113/131114/131115/131116/131117/131118/131119/131120/131121/131122/131123/131124/131125/131126/131127/131128/131129/131130/131131/131
mAP:
waterLily: 0.990909090909091
rose: 0.990909090909091
sunflower: 1.0
lotus: 0.990909090909091
carnation: 0.788888888888889
daffodill: 1.0
magnolia: 0.7850928641251221
camellia: 0.9
dandelion: 0.7251748251748251
daisy: 1.0
mugungwha: 0.9142968142968142
amalea: 0.6499999999999999
moran: 0.9625
cosmos: 0.6041666666666667
lily: 0.622825406758448
pasqueflower: 0.6699999999999999
morning glory: 0.7444444444444445
wild rose: 0.8755555555555555
hydrangea: 0.666735347985348
droplet: 0.7938461538461539
freesia: 0.8527777777777777
2020-06-26 13:41:33,128 - retinanet_logs - INFO - Average accuracy : 0.83
2020-06-26 13:41:33,128 - retinanet_logs - INFO - Average loss 0.28:
